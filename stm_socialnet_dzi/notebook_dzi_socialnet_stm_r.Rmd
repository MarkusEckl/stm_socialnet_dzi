---
title: "Structural Topic Modeling mit Rezensionen aus Socialnet und Artiekl der Soziailen Arbeit aus der DZI Literaturdatenbank"
output:
  html_notebook:
    code_folding: hide
    theme: readable
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
---

```{r}
library(dplyr)
library(tidyr)
library(plyr)
library(magrittr)
library(dplyr)
library(quanteda)
library(spacyr)
library(stm)
library(DT)
library(sna)
library(ggplot2)

setwd("/home/eckl/analyse_socialnet_dzi_stm/")

```

##DZI Datensatz Bereinigung & Vorbereitung 
```{r}
dzi <- read.csv("data/dzi_zsa_finall.csv", sep = ",", encoding = "utf-8")

dzi[["Puplikation_art"]] <- c(rep("Artikel", nrow(dzi)))

dzi$Jahr <- gsub("1998/1999", "1999", dzi$Jahr)
dzi$Jahr <- gsub("20003", "2003", dzi$Jahr)
dzi$Jahr <- gsub("20009", "2003", dzi$Jahr)
dzi$Jahr <- gsub("2ß14", "2014", dzi$Jahr)
summary(dzi$Jahr)

```



##scoialnet Datensatz Bereinigung und Vorbereitung 
```{r}

socnet <- read.csv("data/socialnet_df_finale2.csv", sep = "\t", encoding = "utf-8")

socnet$Erscheinungsjahr <- gsub("2012.0", "2012", socnet$Erscheinungsjahr)
socnet$Erscheinungsjahr <- gsub("2013.0", "2013", socnet$Erscheinungsjahr)
socnet$Erscheinungsjahr <- gsub("2014.0", "2014", socnet$Erscheinungsjahr)
socnet$Erscheinungsjahr <- gsub("2015.0", "2015", socnet$Erscheinungsjahr)
socnet$Erscheinungsjahr <- gsub("2016.0", "2016", socnet$Erscheinungsjahr)
socnet$Erscheinungsjahr <- gsub("2017.0", "2017", socnet$Erscheinungsjahr)
socnet$Erscheinungsjahr <- gsub("2018.0", "2018", socnet$Erscheinungsjahr)
socnet$Erscheinungsjahr <- gsub("978-3-8379-2513-5", NA,socnet$Erscheinungsjahr)

socnet2 <- socnet %>% drop_na(Erscheinungsjahr)
socnet2 <- subset(socnet2, Erscheinungsjahr != "")
socnet2 <- subset(socnet2, Erscheinungsjahr != " ")
socnet2$Inahlt_bereinigt <- as.character(socnet2$Inhalt_bereinigt)
#socnet2$Erscheinungsjahr <- as.integer(socnet2$Erscheinungsjahr)

socnet3 <- select(socnet2, "Erscheinungsjahr", "Inhalt_bereinigt", "Titel","RezNachname")

socnet3[["Puplikation_art"]] <- c(rep("Rezension", nrow(socnet3)))
```


##Zusammenführung der beiden Datensätze
```{r}

socnet3 <- rename(socnet3, c("Erscheinungsjahr"="Jahr", "Inhalt_bereinigt"="Text", "RezNachname"="Autor"))
dzi <- rename(dzi, c("Autor_1"="Autor", "Abstract"="Text"))

df <- merge(socnet3, dzi, all=TRUE)
df$Puplikation_art <- as.factor(df$Puplikation_art)

df$Text <- as.character(df$Text)
#df$Jahr <- as.factor(df$Jahr)


write.csv(df, file = "data/df_dzi_socialnet.csv")

```



```{r}

df0 <- read.csv("data/df_dzi_socialnet.csv", encoding = "utf-8", sep=",")




g <- ggplot(df0, aes(Jahr))
g + geom_bar(aes(fill=Puplikation_art), width = 0.5)  + 
  theme(axis.text.x = element_text(angle=65, vjust=0.6)) + 
  #scale_y_continuous(labels = comma) +
  labs(title="Verteilung der Artikelabstracts und Rezensionen über die Jahre") 

df <- subset(df0, df0$Jahr >= 2000 & df0$Jahr < 2018)
df$Text <- as.character(df$Text)
```
## Textbereinigung  

### POS-Tagging & Lemmatising 
```{r message=FALSE, warning=FALSE, results = 'hide', eval = FALSE}
# Start the clock!


ptm <- proc.time()

spacy_initialize(model = 'de' ,refresh_settings = TRUE)
parsed <- spacy_parse(df$Text)
spacy_finalize()
save.image('output/socialnet_dzi_text_paresed.RData')

proc.time() - ptm

```

```{r message=FALSE, warning=FALSE, results = 'hide', eval = FALSE}
load('output/socialnet_dzi_text_paresed.RData') # loads lemma

tokens <- as.tokens(parsed, use_lemma = TRUE) %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(c(stopwords('de'), "vgl", "et_a1", "fiir","v0n", "a1s", "hinsichtlich", 
                  "11nd", "z._b.", "cine", "hierzu", "erstens", "zweitens", "deutlich", "tion",
                   "geben", "mehr", "immer", "schon", "gehen", "sowie", "erst", "mehr", "etwa",
                  "dabei", "dis-", "beziehungsweise", "seit", "drei", "insbesondere",
                  stopwords("en")),
                min_nchar = 4L,  padding = TRUE)


collocation <- textstat_collocations(tokens, min_count = 30)
tokens <- tokens_compound(tokens, collocation, join = FALSE)


docvars(tokens) <- df %>% select(Text, Jahr, Puplikation_art) 


dfm_sp <- tokens %>% dfm() %>% dfm_select(min_nchar = 4L) %>% 
  dfm_trim(min_docfreq = 50) %>%  # minimum 50 documents (removes rare terms)
   dfm_trim(max_docfreq = 0.25,
            docfreq_type = 'prop') # maximum in 25 percent of documents (stop words)

sp_grouped <- dfm_group(dfm_sp, groups = 'Jahr') # grouped dfm for keyness

str(dfm_sp)

#docnames(dfm_sp)[which(rowSums(dfm_sp) == 0)]

#dfm_sp2 <- dfm_sp[rowSums(dfm_sp) > 0, ]


#docnames(dfm_sp2)[which(rowSums(dfm_sp2) == 0)]

save.image('output/data_cleaning_dzi_socialnet.RData')


```



#Topic Modeling 

```{r}

#https://juliasilge.com/blog/evaluating-stm/

library(stm)
library(furrr)
plan(multiprocess)

many_models <- data_frame(K = c(20,40,60,80,100,120,140,160)) %>%
  mutate(topic_model = future_map(K, ~stm(df, K = .,
                                          verbose = FALSE)))



```


##Evaluation
```{r}


load('output/data_cleaning_dzi_socialnet.RData') # loads  stm model

#topic.count <- 40
dfm2stm <- convert(dfm_sp, to = "stm")


kResult <- searchK(dfm2stm$documents, dfm2stm$vocab, K=c(20,40,60,80,100,120,140,160), data=dfm2stm$meta)
plot(kResult)
kResult2 <- searchK(dfm2stm$documents, dfm2stm$vocab, K=c(180,200,220), data=dfm2stm$meta)
#plot(kResult$results$semcoh)
plot.searchK(kResult2)
plot.searchK(kResult)


###############################################
library(ggplot2)
library(ggrepel)
semantic_coherence <- kResult$results$semcoh
exclusivity <- kResult$results$exclus
topic_model <- kResult$results$K
n_topics <- c("20 Topics", "40 Topics", "60 Topics", "80 Topics","100 Topics","120 Topics", "140 Topics", "160 Topics")
evaluation_var <-data.frame(exclusivity,semantic_coherence, topic_model, n_topics)
evaluation_var

semantic_coherence <- kResult2$results$semcoh
exclusivity <- kResult2$results$exclus
topic_model <- kResult2$results$K
n_topics <- c("180 Topics", "200 Topics", "220 Topics")
evaluation_var2 <-data.frame(exclusivity,semantic_coherence, topic_model, n_topics)
evaluation_var2

evaluation_var_20_220 <- rbind(evaluation_var, evaluation_var2)
evaluation_var_20_220
write.csv(evaluation_var, file = "data/evaluation_var_20_220.csv")



png("evaluation_sementic_coherence_exclusivity.png", width=900, height=600, res=150)
px <- ggplot(evaluation_var_20_220, aes(semantic_coherence, exclusivity)) +
  geom_point(color = 'red')+ 
  geom_label_repel(aes(label = n_topics, 
                       fill = factor(n_topics)), color = 'white',
                   size = 2.5) +
  theme(legend.position = "bottom") +
  labs(title="Semantic coherence und exclusivity der berechneten Modelle", x = "semantic coherence", 
       y = " exclusivity") + labs(fill = "Modelle mit ")
  theme_update(plot.title = element_text(hjust = 0.5))
px
dev.off()
px
save.image('output/stm_evaluation.RData')


```

##Modell

```{r  warning=FALSE, paged.print=FALSE, results='hide'}
topic.count <- 140
#dfm2stm <- convert(dfm_sp, to = "stm")

model.stm <- stm(dfm2stm$documents, 
                 dfm2stm$vocab, 
                 K = topic.count, 
                 data = dfm2stm$meta, 
                 init.type = "Spectral") # this is the actual stm call


save.image('output/stm_model_dzi_socialnet.RData')

```



```{r}

load('output/stm_model_dzi_socialnet.RData') # loads  stm model

#df.topics <- data.frame(t(labelTopics(model.stm, n = 20)$prob))
#df.topics %>% 
#    head(100) %>% DT::datatable(options = list(lengthMenu = c(5, 10, 20)))
library(markdown)
df.topics <- data.frame(t(labelTopics(model.stm, n = 20)$score))
p <- df.topics %>% 
    head(100) %>% DT::datatable(options = list(lengthMenu = c(10, 10, 20)))
print(p)

```


```{r fig.height=15, fig.width=40}
library(markdown)
labelTopics(model.stm, n=10)

```



## Top Topics  
Vergleicht man die Häufigkeit der gefundenen Topics im Corpus dann zeigt sich, dass Topic 19 am häufigsten Auftrat. Mit nur geringem Abstand folgen danach Topic 10, 7, 16, 14 und 12. 
```{r fig.height=25, fig.width=15}
#help(stm)
#plot(model.stm, type = "summary", xlim=(c(0,0.15)), ylim = (c(0,20)), n = 8)

par(bty="n",col="grey40",lwd=3)
plot.STM(model.stm,type="summary",xlim=(c(0,0.05)), ylim = (c(0,140)), n =7)

```


## Wordcloud 
Als nächstes wurden Wordclouds für die jeweiligen Topics erstellt. Daber wurden die 20 Wörter eines Topics herangezogen, welche die höchste Wahrscheinlichkeit haben, das Topic zu repräsentieren. 
```{r fig.height=12, fig.width=15}
library(RColorBrewer)
library(wordcloud)


model.stm.labels$labname <- c(rep("Topic", 140))


topic.count <- 140
par(mfrow=c(3,3))
for (i in seq_along(sample(1:topic.count, size = topic.count)))
{
  cloud(model.stm, topic = i, scale = c(4,.40), 
        max.words = 20, main = paste0("Topic ", model.stm.labels$topicnums[i],  collapse = ", "))
}
```

## Topic Vergleich
```{r fig.height=8, fig.width=10}

 c(2,4,7,15,11,10)
plot(model.stm, type = "perspectives", topics = c(2,4), n = 30)
plot(model.stm, type = "perspectives", topics = c(7,15), n = 30)
plot(model.stm, type = "perspectives", topics = c(11,10), n = 30)

```




```{r fig.height=12, fig.width=15}


#topic.count =80


model.stm.labels <- labelTopics(model.stm, 1:topic.count)
dfm2stm$meta$datum <- as.numeric(dfm2stm$meta$Jahr)
model.stm.ee <- estimateEffect(1:topic.count ~  s(Jahr), model.stm, meta = dfm2stm$meta)


par(mfrow=c(3,3))
for (i in seq_along(sample(1:topic.count, size = topic.count)))
{
  plot(model.stm.ee, "Jahr", method = "continuous", topics = i, main = paste0("Topic ",                              model.stm.labels$topicnums[i], ": ", model.stm.labels$labname[i]), printlegend = F, xlim = c(2000, 2017), ylim= c(0.001,0.03))
}

```




```{r}

#topic.count = 20


topic.Interation <- stm(dfm2stm$documents, 
                 dfm2stm$vocab, 
                 K = 10, 
                 prevalence =  ~ Puplikation_art * Jahr,
                 data = dfm2stm$meta, 
                 init.type = "Spectral") 
str(topic.Interation)

summary(dfm2stm$meta$Puplikation_art)

library(stminsights)


prep <- estimateEffect(c(6) ~ Puplikation_art * s(Jahr), topic.Interation, metadata = dfm2stm$meta, uncertainty = "Global")

effects <- get_effects(estimates = prep, 
                       variable = "Jahr",
                       type = "continuous")
str(effects)

prep$data$Puplikation_art <- as.numeric(prep$data$Puplikation_art)
summary(prep$data$Puplikation_art)

```



```{r}

#topic.count = 20



topic.Interation <- stm(dfm2stm$documents, 
                 dfm2stm$vocab, 
                 K = topic.count, 
                 prevalence =  ~ Puplikation_art + s(Jahr),
                 data = dfm2stm$meta, 
                 init.type = "Spectral") 
str(topic.Interation)

summary(dfm2stm$meta$Puplikation_art)

prep <- estimateEffect(c(16) ~ Puplikation_art * s(Jahr), topic.Interation, metadata = dfm2stm$meta, uncertainty = "Global")
summary(prep)


plot(prep, covariate = "Jahr", model = topic.Interation, method = "continuous", xlab = "Jahr", moderator = "Puplikation_art", 
     moderator.value = "Artikel", linecol = "blue", printlegend = F , ylim = c(0.006, 0.05) )

plot(prep, covariate = "Jahr", model = topic.Interation, method = "continuous", xlab = "Jahr", moderator = "Puplikation_art", 
     moderator.value = "Rezension", linecol = "red", add = T, printlegend = F, ylim = c(0.001, 0.05)) 
legend(c("Artikel", "Rezension"), lwd=2, col=c("blue", "red"))



plot(prep, covariate = "Puplikation_art", topics = 16, model = topic.Interation, method = "difference",
     cov.value1 = "Rezension", cov.value2 = "Artikel", xlim =c(-0.03,0.05), xlab = "Artikel vs. Rezension", labeltype = "custom", custom.labels = "topic 16")
```



```{r fig.height=20, fig.width=20}
library(stminsights)
library(shiny)
library(shinydashboard)
library(ggraph)
stm_corrs <- get_network(model = model.stm,
                         method = 'simple',
                         labels = paste('Topic', 1:20),
                         cutoff = 0.1,
                         cutiso = TRUE)

graph <-ggraph(stm_corrs, layout = 'fr') +
  geom_edge_link(
    aes(edge_width = weight),
    label_colour = '#fc8d62',
    edge_colour = '#377eb8') +
  geom_node_point(size = 6, colour = 'black')  +
  geom_node_label(
    aes(label = name, size = props),
    colour = 'black',  repel = TRUE, alpha = 0.85) +
  scale_size(range = c(5, 13), labels = scales::percent) +
  labs(size = 'Topic Proportion',  edge_width = 'Topic Correlation') +
  scale_edge_width(range = c(2, 9)) +
  theme_graph()

graph


```


